# ğŸ“¦ Livrables - Projet dâ€™analyse de donnÃ©es pandÃ©miques

## 1. ModÃ¨le de donnÃ©es UML

ğŸ“ Fichier : `csv_to_postgres/schema BDD.svg`

ReprÃ©sentation complÃ¨te du schÃ©ma relationnel de la base de donnÃ©es utilisÃ©e pour centraliser les diffÃ©rentes sources de donnÃ©es.

---

## 2. Base de donnÃ©es relationnelle & script de crÃ©ation

Afin de crÃ©er la base de donnÃ©es PostgreSQL, veuillez tÃ©lÃ©charger la suite logicielle PostgreSQL :

ğŸ‘‰ [PostgreSQL - EnterpriseDB](https://www.enterprisedb.com/downloads/postgres-postgresql-downloads)

### Ã‰tapes de mise en place :

1. Installer PostgreSQL et pgAdmin.
2. Lancer pgAdmin et crÃ©er un **nouveau serveur**.
    - Exemple :
        - Nom du serveur : `localhost`
        - Hostname : `localhost`
        - Username : `postgres` ou celui dÃ©fini Ã  l'installation de PostgreSQL
        - Mot de passe : celui dÃ©fini Ã  lâ€™installation de PostgreSQL
3. CrÃ©er une **nouvelle base de donnÃ©es** nommÃ©e `MSPR`.

---

## 3. Mise en fonctionnement de lâ€™Apollo Server

### Configuration des variables dâ€™environnement

Avant de lancer le serveur, configurez les variables dâ€™environnement nÃ©cessaires Ã  la connexion Ã  la base de donnÃ©es PostgreSQL.

1. Copiez le fichier `.env.example` en `.env` dans le dossier `backend` :

```bash
cp .env.example .env
```

2. Ouvrez le fichier `.env` et remplacez les valeurs par dÃ©faut par vos informations rÃ©elles, notamment le **mot de passe** et le **nom dâ€™utilisateur** PostgreSQL si vous les avez modifiÃ©s lors de lâ€™installation.

Exemple de section Ã  modifier dans `.env`Â :

```
DATABASE_URL="postgresql://<utilisateur>:<mot_de_passe>@localhost:5432/MSPR?schema=public"
```

Assurez-vous que ces informations correspondent Ã  celles de votre instance PostgreSQL.

### Installation des dÃ©pendances

Depuis le dossier `backend` :

```bash
npm install
```

### DÃ©ploiement des migrations Prisma

```bash
npx prisma migrate deploy
```

### GÃ©nÃ©ration du client Prisma

```bash
npx prisma generate
```

### Lancement du serveur

```bash
npm run start
```

---

## 4. Importation des donnÃ©es dans la base

Depuis le dossier `csv_to_postgres` :

1. CrÃ©er un environnement virtuel :

```bash
py -m venv venv
```

1. Installer les dÃ©pendances :

```bash
pip install -r requirements.txt
```

1. Lancer le script dâ€™importation :

```bash
py migration-script.py
```

---

## 5. Code ETL

ğŸ“ Notebook Jupyter : `etl/data_cleaning.ipynb`

Contient lâ€™ensemble du processus de prÃ©paration, nettoyage et fusion des donnÃ©es brutes.

---

## 6. API CRUD flexible (GraphQL)

CrÃ©Ã©e avec **GraphQL** pour permettre des requÃªtes souples et ciblÃ©es sur les donnÃ©es nettoyÃ©es.

---

## 7. Documentation API (OpenAPI Spec)

[Ã€ complÃ©ter]

---

## 8. Tableau de bord interactif

Un outil de datavisualisation permet :

- Lâ€™exploration des donnÃ©es historiques
- Lâ€™exportation des visualisations
- Lâ€™application de filtres pour faciliter la lecture

Les choix de filtres seront justifiÃ©s dans la documentation.

[Outil utilisÃ© : Power BI ?]

---

## 9. Documentation dÃ©taillÃ©e : collecte et nettoyage des donnÃ©es

RÃ©sumÃ© du notebook :

> La fusion des jeux de donnÃ©es repose sur une normalisation prÃ©alable de chaque source pour garantir une homogÃ©nÃ©itÃ© avant la fusion. Cela permet de minimiser les incohÃ©rences et de limiter le nettoyage post-fusion.
> 

### Ã‰tapes clÃ©s :

1. Suppression des colonnes inutiles et harmonisation des noms
2. Gestion des valeurs manquantes (suppression ou imputation)
3. Formatage des types (dates, float, etc.)
4. VÃ©rification de la cohÃ©rence intercolonnes
5. Suppression des doublons intradataset
6. Fusion des datasets
7. DÃ©tection de doublons interdatasets aprÃ¨s fusion
8. Feature Engineering :
    - Taux de croissance des cas
    - Taux de mortalitÃ©
    - % de population touchÃ©e
    - SaisonnalitÃ©
    - Moyennes mondiales de rÃ©fÃ©rence

---

## 10. Benchmark des solutions techniques

### ğŸ“Œ 1. **ETL & PrÃ©paration**

- `Pandas` â†’ RÃ©fÃ©rence en traitement CSV
- `Jupyter Notebook` â†’ ItÃ©ration rapide et visualisation instantanÃ©e

---

### ğŸ“Œ 2. **Stockage des donnÃ©es**

- **PostgreSQL** âœ… â†’ Choix principal, robuste & intÃ©grÃ©
- **Alternatives** : MySQL, DuckDB, ClickHouse, BigQuery
- **ORMs** :
    - Prisma (Node.js) â†’ ORM typÃ©, moderne et multi-SGBD
    - SQLAlchemy (Python) â†’ IntÃ©grÃ© pour lâ€™export depuis le notebook

---

### ğŸ“Œ 3. **API & CRUD**

- **GraphQL** â†’ RequÃªtage ciblÃ© et structurÃ©
- **REST (FastAPI, Express.js)** â†’ Standard mais moins souple
- **Librairies** : Apollo Server (Node.js)

---

### ğŸ“Œ 4. **Tests dâ€™API**

- **Postman** â†’ Tests automatisÃ©s
- **Insomnia** â†’ Alternative lÃ©gÃ¨re

---

### ğŸ“Œ 5. **Visualisation des donnÃ©es**

- `Matplotlib`, `Seaborn` â†’ Explorations initiales
- `Plotly`, `Bokeh` â†’ Graphiques interactifs
- `Power BI` â†’ RecommandÃ© pour filtres, export, et dashboards clairs
- `Apache Superset`, `Metabase` â†’ Solutions open-source

---

## 11. Diagramme de Gantt

ğŸ“Œ Conseil : construisez-le au fil de lâ€™avancement dans **Notion**, en attribuant les tÃ¢ches par contributeur pour suivre efficacement le projet.